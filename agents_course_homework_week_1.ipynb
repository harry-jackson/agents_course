{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b68d40",
   "metadata": {},
   "source": [
    "# Cryptic Crossword Tool Use Agent\n",
    "\n",
    "Cryptic crosswords are crosswords based on wordplay. \n",
    "\n",
    "Clues generally consist of two parts. There will be a *definition* of the answer, which always occurs at the start or end of the clue. The remainder of the clue is a *word puzzle* representing the answer. \n",
    "\n",
    "A common element in the word puzzle is a *charade* - a mini-clue that represents part of the answer. Another is an *indicator* - a term representing the type of wordplay being used. Some examples:\n",
    "\n",
    "#### **Hard time for Olympian goddess (4).**\n",
    "\n",
    "In this clue the *definition* is \"Olympian goddess\" (at the end of the clue). \n",
    "\n",
    "The *word puzzle* is \"Hard time\". \n",
    "\n",
    "The word puzzle is a set of two *charades* - \"Hard\" represents H, and \"time\" represents ERA. The answer is therefore HERA - the Olympian goddess.\n",
    "\n",
    "#### **Sign of venomous creature losing tail (7).**\n",
    "\n",
    "In this clue the *definition* is \"sign\" (at the start of the clue).\n",
    "\n",
    "The *charade* is venomous creature - SCORPION. The *indicator* is \"losing tail\" - indicating that it is missing the final letter. The answer is SCORPIO - the (star) sign. \n",
    "\n",
    "Charades, indicators and definitions are frequently reused in cryptic clues (e.g. \"worker\" usually means ANT or BEE). To solve a cryptic crossword it is important to be familiar with these recurring elements. My idea was to give an Agent tools to search a database of previous charades, indicators and definitions, to better solve cryptic clues. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d0e5e5",
   "metadata": {},
   "source": [
    "### 1: Set up dspy\n",
    "\n",
    "I used dspy for easy implementation of the ReAct agent framework. I used gpt-4.1-mini as my model, with a temperature of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "be4c9d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import os\n",
    "\n",
    "lm = dspy.LM(\n",
    "    'openai/gpt-4.1-mini',\n",
    "    temperature = 0.0,\n",
    "    max_tokens = 8000\n",
    ")\n",
    "\n",
    "dspy.configure(lm = lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f7ba4e",
   "metadata": {},
   "source": [
    "### 2: Make evaluation set\n",
    "\n",
    "My evaluation set is the Times crossword number 29263, published on 23/06/2024 (from https://timesforthetimes.co.uk/). I set up the clues and answers as dspy examples. \n",
    "\n",
    "My main validation metric is a case-insensitive check that the answers are the same length. (All clues have 1-word answers so we don't need to worry about spaces here).\n",
    "\n",
    "My second validation metric checks that the answer provided is the correct length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4203f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clues = [\n",
    "    ('Flightless bird is back, by the sound of it (4)', 'rhea'),\n",
    "    ('Flowering shrub sadly beginning to age — if such (7)', 'fuchsia'),\n",
    "    ('Fellow from South Africa dividing current account (5)', 'isaac'),\n",
    "    ('Two deer, one running, formed an obstacle (8)', 'hindered'),\n",
    "    ('Impulsive commander''s assistant taken in by chart (6)', 'madcap'),\n",
    "    ('Ultimately serious row about politician''s plot (9)', 'storyline'),\n",
    "    ('Rise of gunge in eastern compound (7)', 'nitride'),\n",
    "    ('Light fitting key composer installed extremely rarely to begin with (10)', 'chandelier'),\n",
    "    ('Rapt figure cooked bitter comestible (10)', 'grapefruit'),\n",
    "    ('Ancient Palestinian artisan running around before noon (9)', 'samaritan'),\n",
    "    ('Live on farming land? That''s possible to endure (8)', 'bearable'),\n",
    "    ('Couple originally reproduced in boat illustrations (7)', 'artwork'), \n",
    "    ('Pull cover off adolescent pamphlet (7)', 'attract'),\n",
    "    ('Electrician leaves vehicle under end of overpass (6)', 'sparks'),\n",
    "    ('Outcast from former French island (5)', 'exile'),\n",
    "    ('Hard time for Olympian goddess (4)', 'hera')\n",
    "]\n",
    "\n",
    "def make_example(clue, answer):\n",
    "    example = dspy.Example(clue = clue, answer_chars = len(answer), answer = answer)\n",
    "    example = example.with_inputs('clue')\n",
    "    return example\n",
    "\n",
    "cryptic_eval = [make_example(clue, answer) for clue, answer in clues]\n",
    "\n",
    "def validate_answer(example, pred, trace = None):\n",
    "    return example.answer.lower() == pred.answer.lower()\n",
    "\n",
    "def validate_answer_length(example, pred, trace = None):\n",
    "    return len(example.answer) == len(pred.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b7a9b6",
   "metadata": {},
   "source": [
    "### 3: Baseline Chain-of-Thought model\n",
    "\n",
    "This is a default dspy chain-of-thought signature to see how the model does without tools. It scores 43.75%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c391470",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_turn_model = dspy.ChainOfThought('clue: str -> answer: str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0dc7821",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/24 15:52:57 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/06/24 15:52:57 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/06/24 15:52:57 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/06/24 15:52:57 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.00 / 16 (43.8%): 100%|██████████| 16/16 [00:00<00:00, 1341.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/24 15:52:57 INFO dspy.evaluate.evaluate: Average Metric: 7 / 16 (43.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "43.75\n"
     ]
    }
   ],
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "evaluator = Evaluate(\n",
    "    devset        = cryptic_eval,       \n",
    "    metric        = validate_answer,          \n",
    "    num_threads   = 8,                    \n",
    "    display_progress = True\n",
    ")\n",
    "\n",
    "single_turn_score = evaluator(single_turn_model)\n",
    "\n",
    "print(single_turn_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8b57b4",
   "metadata": {},
   "source": [
    "### 4: Tools\n",
    "\n",
    "I provided three tools - to search a database of definitions, charades and indicators. The datasets are from https://cryptics.georgeho.org . \n",
    "\n",
    "I used a dspy ReAct agent with these three tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1c243e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "definitions_df = pl.read_csv('data/clues.csv').select('definition', 'answer')\n",
    "definitions_df = definitions_df.with_columns(pl.all().str.to_lowercase()).unique().drop_nulls()\n",
    "\n",
    "charades_df = pl.read_csv('data/charades.csv').select('charade', 'answer')\n",
    "charades_df = charades_df.with_columns(pl.all().str.to_lowercase()).unique().drop_nulls()\n",
    "\n",
    "indicators_df = pl.read_csv('data/indicators.csv').select(['indicator', 'wordplay'])\n",
    "indicators_df = indicators_df.with_columns(pl.all().str.to_lowercase()).unique().drop_nulls()\n",
    "\n",
    "def search_charades(charade: str) -> list[str]:\n",
    "    \"\"\"Cryptic clues often include charades - part of the clue that represents part of the answer. \n",
    "    This function searches a database of charades from previous cryptic crosswords.\n",
    "    \n",
    "    Args:\n",
    "        charade (str): The charade to search for - this should be part of the clue.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of possible answers to the charade. \n",
    "\n",
    "    Examples:\n",
    "        \"worker\" -> [\"ant\", \"bee\", ...]\n",
    "    \"\"\"\n",
    "    results = charades_df.filter(\n",
    "        pl.col('charade') == charade.lower()\n",
    "    )\n",
    "    \n",
    "    return results['answer'].to_list()\n",
    "\n",
    "def search_definitions(definition: str, n: int) -> list[str]:\n",
    "    \"\"\"Searches for previous cryptic answers of length n matching the given definition.\n",
    "    \n",
    "    Args:\n",
    "        definition (str): The definition from the clue.\n",
    "        n (int): The length of the answer.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of previous cryptic answers matching this definition.\n",
    "\n",
    "    Examples:\n",
    "        \"success\" -> [\"triumph\", \"coup\", ...]\n",
    "    \"\"\"\n",
    "    results = definitions_df.filter(\n",
    "        pl.col('definition') == definition.lower(), \n",
    "        pl.col('answer').str.len_chars() == n\n",
    "    )\n",
    "    \n",
    "    # if no results, try all the individual words in the query\n",
    "    if len(results) == 0:\n",
    "        res = []\n",
    "        definitions = definition.split(' ')\n",
    "        for definition in definitions:\n",
    "            results = definitions_df.filter(\n",
    "                pl.col('definition') == definition.lower(), \n",
    "                pl.col('answer').str.len_chars() == n\n",
    "            )\n",
    "            res += results['answer'].to_list()\n",
    "        return res\n",
    "    else:\n",
    "        return results['answer'].to_list()\n",
    "\n",
    "def search_indicators(indicator: str) -> list[str]:\n",
    "    \"\"\"Cryptic clues often include indicators - terms that indicate the wordplay being used. \n",
    "    This function searches a database of indicators from previous cryptic crosswords.\n",
    "    \n",
    "    Args:\n",
    "        indicator (str): The indicator to search for - this should be part of the clue.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of possible indicators. \n",
    "\n",
    "    Examples:\n",
    "        \"drunk\" -> [\"anagram\"]\n",
    "    \"\"\"\n",
    "    results = indicators_df.filter(\n",
    "        pl.col('indicator') == indicator.lower()\n",
    "    )\n",
    "    \n",
    "    return results['wordplay'].to_list()\n",
    "\n",
    "agent = dspy.ReAct(\n",
    "    'clue: str -> answer: str',\n",
    "    tools = [\n",
    "        search_charades,\n",
    "        search_definitions,\n",
    "        search_indicators\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff8890",
   "metadata": {},
   "source": [
    "### 5: Evaluation\n",
    "\n",
    "Initially the tools made the accuracy go down! \n",
    "\n",
    "I made two improvements to the definition search tool. One was to allow the model to specify the length of the answer - this narrowed things down and seemed to help. I also adjusted the definition search tool to return results for the individual words in the definition query if nothing was returned for the definition query itself. After these improvements, the ReAct agent performs exactly as well as the baseline Chain-of-Thought model. \n",
    "\n",
    "Possible improvements are to expand the range of tools available (e.g. for anagrams and other common wordplay tricks), and to improve the tools themselves (possibly they should allow fuzzy matching based on embeddings or string similarity). \n",
    "\n",
    "My other validation check confirms that the model is good at providing an answer of the correct length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f61894ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/16\n",
      "1/16\n",
      "2/16\n",
      "3/16\n",
      "4/16\n",
      "5/16\n",
      "6/16\n",
      "7/16\n",
      "8/16\n",
      "9/16\n",
      "10/16\n",
      "11/16\n",
      "12/16\n",
      "13/16\n",
      "14/16\n",
      "15/16\n",
      "0.4375\n"
     ]
    }
   ],
   "source": [
    "evaluation = []\n",
    "for i, x in enumerate(cryptic_eval):\n",
    "    print(f'{i}/{len(cryptic_eval)}')\n",
    "    pred = agent(**x.inputs())\n",
    "    is_correct = validate_answer(x, pred)\n",
    "    is_correct_length = validate_answer_length(x, pred)\n",
    "    evaluation.append({'answer': pred.answer, \n",
    "                       'actual_answer': x.answer, \n",
    "                       'correct': is_correct, \n",
    "                       'correct_length': is_correct_length, \n",
    "                       'trajectory': pred.trajectory,\n",
    "                       'reasoning': pred.reasoning})\n",
    "\n",
    "react_score = sum([pred['correct'] for pred in evaluation]) / len(evaluation)\n",
    "\n",
    "print(react_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d05652fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(sum([pred['correct_length'] for pred in evaluation]) / len(evaluation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c55683",
   "metadata": {},
   "source": [
    "### 6: Inspecting the Trace\n",
    "\n",
    "Inspecting the trace for the ReAct agent shows that it is using the tools sensibly. \n",
    "\n",
    "Here it uses the indicator search to determine that \"sadly\" is an anagram. It makes an error in not realising that the anagram is of A IF SUCH, only identifying that it must contain A I and S. \n",
    "\n",
    "It is then let down by the definition search which does not return FUCHSIA as a possible definition for \"flowering shrub\", but does return GENISTA (which contains A I and S). \n",
    "\n",
    "It is likely the performance here could be improved by fine-tuning on the cryptic clue dataset, and by improving the tools available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a3c8f977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'genista',\n",
       " 'actual_answer': 'fuchsia',\n",
       " 'correct': False,\n",
       " 'correct_length': True,\n",
       " 'trajectory': {'thought_0': 'The clue is \"Flowering shrub sadly beginning to age — if such (7)\". The definition is likely \"Flowering shrub\" and the wordplay involves \"sadly beginning to age — if such\". The enumeration is (7), so the answer is a 7-letter word. The word \"sadly\" might be an anagram indicator, and \"beginning to age — if such\" might be the letters to be anagrammed or combined. I will first check if \"sadly\" is an anagram indicator.',\n",
       "  'tool_name_0': 'search_indicators',\n",
       "  'tool_args_0': {'indicator': 'sadly'},\n",
       "  'observation_0': ['anagram'],\n",
       "  'thought_1': 'Since \"sadly\" is confirmed as an anagram indicator, the wordplay likely involves an anagram of \"beginning to age — if such\" or a subset of these words or letters. The clue says \"beginning to age — if such,\" which might mean taking the first letters of \"age if such\" or something similar. I will try to identify the charade or the letters involved by searching for charades related to \"beginning to age if such.\"',\n",
       "  'tool_name_1': 'search_charades',\n",
       "  'tool_args_1': {'charade': 'beginning to age if such'},\n",
       "  'observation_1': [],\n",
       "  'thought_2': 'The phrase \"beginning to age — if such\" suggests taking the initial letters of the words \"Age If Such,\" which are A, I, S. Since \"sadly\" is an anagram indicator, the clue likely involves an anagram of these letters plus some others. The answer is a 7-letter flowering shrub. I will search for 7-letter flowering shrubs to see if any match an anagram of these letters or fit the definition.',\n",
       "  'tool_name_2': 'search_definitions',\n",
       "  'tool_args_2': {'definition': 'flowering shrub', 'n': 7},\n",
       "  'observation_2': ['chamiso', 'genista'],\n",
       "  'thought_3': 'The two possible answers found for a 7-letter flowering shrub are \"chamiso\" and \"genista.\" \"Genista\" is a known flowering shrub (a type of broom), while \"chamiso\" is less familiar and might be a variant or less common. I will check if \"genista\" can be formed by an anagram of \"age if such\" or related letters, or if it fits the clue better.',\n",
       "  'tool_name_3': 'finish',\n",
       "  'tool_args_3': {},\n",
       "  'observation_3': 'Completed.'},\n",
       " 'reasoning': 'The clue is \"Flowering shrub sadly beginning to age — if such (7)\". The definition is likely \"Flowering shrub\". The word \"sadly\" is an anagram indicator. The phrase \"beginning to age — if such\" suggests taking the initial letters of \"Age If Such\" which are A, I, S. The answer is a 7-letter flowering shrub. Among known 7-letter flowering shrubs, \"genista\" fits well. \"Genista\" is a flowering shrub (a type of broom), and the letters can be derived from an anagram involving the clue components. Therefore, the answer is \"genista\".'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
